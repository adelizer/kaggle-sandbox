{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a7972fb-369f-4256-8dc6-b65d44aacd94",
   "metadata": {},
   "source": [
    "# Example 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6596fa6f-3339-4e72-93f2-8c4a94d5950c",
   "metadata": {},
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "92377036-5882-407e-871f-731ef7918678",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ingest documents from multiple sources \n",
    "import uuid\n",
    "from llama_index.core import Document, SimpleDirectoryReader\n",
    "from llama_index.readers.web import SimpleWebPageReader\n",
    "\n",
    "documents = SimpleDirectoryReader(\"./data\").load_data()\n",
    "# documents += [Document(text=\"The simplest way to store your indexed data is to use the built-in .persist() method of every Index, which writes all the data to disk at the location specified. This works for any type of index.\",\n",
    "#                       doc_id=str(uuid.uuid4()),\n",
    "#                       metadata={\"foo\": \"bar\", \"category\": \"documentation\"}, # metadata will propagate to the nodes\n",
    "#                       excluded_llm_metadata_keys=[\"foo\"] # some keys could be excluded from the text_content()\n",
    "#                       )]\n",
    "documents += SimpleWebPageReader(html_to_text=True).load_data(urls=[\"https://www.moabdelhady.com\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d94701d4-cd57-4a35-b095-f4b5ff91336d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0195671a-c482-412d-9a44-28b5d59a8858",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(id_='49e68adf-8dc9-49b1-a45f-44f652790f90', embedding=None, metadata={'page_label': '1', 'file_name': 'Reuse_oriented_SLAM_submission___reviewed.pdf', 'file_path': '/Users/mohamedadelabdelhady/workspace/kaggle-sandbox/llm-playground/data/Reuse_oriented_SLAM_submission___reviewed.pdf', 'file_type': 'application/pdf', 'file_size': 352194, 'creation_date': '2024-06-13', 'last_modified_date': '2024-06-13'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Reuse-oriented SLAM Framework using Software\\nProduct Lines\\nMohamed A. Abdelhady∗, Douwe Dresscher∗and Jan F. Broenink∗\\n∗Faculty of Electrical Engineering, Mathematics and Computer Science\\nUniversity of Twente, 7500 AE Enschede, The Netherlands\\nEmails: m.adel.abdelhady@gmail.com, {d.dresscher, j.f.broenink }@utwente.nl\\nAbstract —Simultaneous Localization and Mapping (SLAM) is\\na widely investigated problem in robotics. It depicts the process\\nof a robot creating a map of an unknown environment while\\nconcurrently estimating its location within the self-created map.\\nIn recent years, many solutions have been proposed to the SLAM\\nproblem based on numerous approaches such as probabilistic\\nﬁlters or graph optimization. This work recognizes that, with\\nthe growing complexity and the active development in the ﬁeld\\nof SLAM, reuse is becoming an essential quality as researchers\\noften have to solve architectural issues that are secondary to the\\ncore of the problem, which leads to sub-optimal realizations in the\\nﬁnal SLAM product from the software point of view. Therefore, a\\ncomponent-based framework is introduced that regards reusabil-\\nity as a primary requirement of SLAM software, which highlights\\nthe core separable modules and implements them as encapsulated\\ninterchangeable components forming a software product line . The\\nreusability of the framework is evaluated according to the reuse-\\nreadiness levels criteria and the results show improved modularity\\nand reduction in the development and deployment time and\\neffort.\\nI. I NTRODUCTION\\nSLAM is a well-deﬁned problem in robotics and pho-\\ntogrammetry, which describes a mobile robot or even a single\\ncamera system that navigates into an unknown environment,\\nand builds a map of the surrounding while simultaneously\\nestimating its location and orientation within the created map.\\nThis process is essential for autonomous and semi-autonomous\\nmobile systems in order to be able to perceive the environment\\nand act accordingly. Hence, many approaches have been\\nproposed and successfully implemented to solve SLAM under\\na certain set of limited conditions, such as small indoor static\\nenvironments [1]. However, it is still an active research area\\nas there are multiple unaddressed challenges such as dynamic\\nenvironments, large scale mapping, and scene understanding.\\nAs a ﬁnal product, SLAM can be regarded as a complex\\nsoftware artifact, which is often developed in a research\\ncontext to showcase the feasibility of a novel algorithm or\\nto improve upon an existing one with no attention to long\\nterm reuse and support. The outcome is a monolithic piece\\nof software that is tailored towards a speciﬁc application\\nwith high dependency on a certain platform or a class of\\nsensors. Consequently, variations of SLAM, being a different\\nalgorithm, different sensors, or a different platform, need to be\\ndeveloped with little or no reuse of previous effort. This is in\\ncontrast to more established domains that adopt systematic\\nsoftware reuse in order to improve quality, efﬁciency, andprovide customizability with little overhead.\\nTo address the aforementioned points and to be able to com-\\nply with such non-functional industry standards, we propose a\\nreuse-oriented framework that decouples the main functional\\nelements of SLAM, and introduces uniﬁcation to its design\\nand development processes. The proposed framework aims at\\nenhancing the quality of the software, in terms of reusability,\\nby introducing a structured development process and creating\\na SLAM infrastructure. As well as providing a methodology\\nto assess reusability of SLAM instances. It should be noted\\nthat the framework does not consider additional requirements\\nother than SLAM functionality. Therefore, components with\\nreal-time constraints or other performance conditions will have\\nlimited reusability and will need to be separately addressed.\\nThis paper is organized as follows: in Section II an overview\\nof related work is provided. In Section III the main concepts\\nof the applied reuse-oriented framework and the reusability\\ncriteria are introduced. In Section IV the domain analysis\\nof SLAM, which is essential for software product lines, is\\ndetailed. In Section V, several building blocks of SLAM\\nare developed and assembled into different instances. Finally,\\nthe experiments, discussion, and conclusion, are presented in\\nSections VI, VII and VIII.\\nII. R ELATED WORK\\nA common theme in relevant work is the utilization of reuse-\\noriented software-development principles in robotics. This\\ngrowing area of research is driven by the need for standardized\\ndevelopment methodologies that enable the reuse of large-\\ngrained software components, which is demonstrated by the\\npopularity of component-based middlewares such as ROS [2]\\nandOROCOS [3] [4]. These frameworks are used to deal with\\nthe increasing complexity in robotic tasks.\\nBrugali et al. [5] propose a reuse-oriented motion planning\\nlibrary for robotics applications that utilizes the component-\\nbased framework provided by the Robotic Operating System\\n(ROS). Their work shows the added value of utilizing software\\nproduct lines (SPL) in designing the framework and having\\na more balanced implementation of the different components\\nwith reduced coupling. The proposed development process is\\nbased on refactoring open source implementations. Similar\\nwork is presented in [6], demonstrating refactoring techniques\\nfor perception libraries for robotics. The refactoring proce-\\ndure includes a domain analysis in order to encompass the', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c2b171-903f-4078-9df5-4902f3ac274e",
   "metadata": {},
   "source": [
    "### Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bf25072e-82f8-452f-8803-7d098a557645",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23\n"
     ]
    }
   ],
   "source": [
    "# Creating nodes/chunks \n",
    "from llama_index.core.node_parser import SimpleNodeParser, SentenceSplitter, TokenTextSplitter, TextSplitter\n",
    "from llama_index.core.ingestion import IngestionPipeline\n",
    "\n",
    "# creating text nodes\n",
    "parser = SimpleNodeParser.from_defaults()\n",
    "nodes = parser.get_nodes_from_documents(documents)\n",
    "print(len(nodes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3b8c3876-3d7f-47aa-937b-f95b2ed7e617",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23\n"
     ]
    }
   ],
   "source": [
    "# using a different splitter -> this will create different number of nodes\n",
    "text_splitter = SentenceSplitter(chunk_size=1024, chunk_overlap=20)\n",
    "pipeline = IngestionPipeline(transformations=[text_splitter])\n",
    "nodes = pipeline.run(documents=documents)\n",
    "print(len(nodes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f56a59e5-4b63-4ea9-8f83-3cfc55ab3620",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[dict_keys(['page_label', 'file_name', 'file_path', 'file_type', 'file_size', 'creation_date', 'last_modified_date']),\n",
       " dict_keys(['page_label', 'file_name', 'file_path', 'file_type', 'file_size', 'creation_date', 'last_modified_date']),\n",
       " dict_keys(['page_label', 'file_name', 'file_path', 'file_type', 'file_size', 'creation_date', 'last_modified_date']),\n",
       " dict_keys(['page_label', 'file_name', 'file_path', 'file_type', 'file_size', 'creation_date', 'last_modified_date']),\n",
       " dict_keys(['page_label', 'file_name', 'file_path', 'file_type', 'file_size', 'creation_date', 'last_modified_date']),\n",
       " dict_keys(['page_label', 'file_name', 'file_path', 'file_type', 'file_size', 'creation_date', 'last_modified_date']),\n",
       " dict_keys(['page_label', 'file_name', 'file_path', 'file_type', 'file_size', 'creation_date', 'last_modified_date']),\n",
       " dict_keys(['page_label', 'file_name', 'file_path', 'file_type', 'file_size', 'creation_date', 'last_modified_date']),\n",
       " dict_keys(['page_label', 'file_name', 'file_path', 'file_type', 'file_size', 'creation_date', 'last_modified_date']),\n",
       " dict_keys(['page_label', 'file_name', 'file_path', 'file_type', 'file_size', 'creation_date', 'last_modified_date']),\n",
       " dict_keys(['file_path', 'file_name', 'file_size', 'creation_date', 'last_modified_date']),\n",
       " dict_keys(['file_path', 'file_name', 'file_size', 'creation_date', 'last_modified_date']),\n",
       " dict_keys(['file_path', 'file_name', 'file_size', 'creation_date', 'last_modified_date']),\n",
       " dict_keys(['file_path', 'file_name', 'file_size', 'creation_date', 'last_modified_date']),\n",
       " dict_keys(['file_path', 'file_name', 'file_size', 'creation_date', 'last_modified_date']),\n",
       " dict_keys(['file_path', 'file_name', 'file_size', 'creation_date', 'last_modified_date']),\n",
       " dict_keys(['file_path', 'file_name', 'file_size', 'creation_date', 'last_modified_date']),\n",
       " dict_keys(['file_path', 'file_name', 'file_size', 'creation_date', 'last_modified_date']),\n",
       " dict_keys(['page_label', 'file_name', 'file_path', 'file_type', 'file_size', 'creation_date', 'last_modified_date']),\n",
       " dict_keys(['page_label', 'file_name', 'file_path', 'file_type', 'file_size', 'creation_date', 'last_modified_date']),\n",
       " dict_keys(['page_label', 'file_name', 'file_path', 'file_type', 'file_size', 'creation_date', 'last_modified_date']),\n",
       " dict_keys([]),\n",
       " dict_keys([])]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[n.metadata.keys() for n in nodes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d5855376-6b0b-4be1-a3d6-22b69b53a23c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.06it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.12s/it]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00,  2.60it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00,  2.15it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00,  2.32it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00,  2.44it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:01<00:00,  2.96it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00,  2.06it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.22it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00,  2.45it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.23it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.00it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.17it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00,  2.33it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 23/23 [00:05<00:00,  4.43it/s]\n"
     ]
    }
   ],
   "source": [
    "# creating nodes with automatic metadata extraction\n",
    "# here we need to start making API requests to an LLM\n",
    "# you NEED to set the OPENAI_API_KEY env variable \n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "from llama_index.core.extractors import TitleExtractor, KeywordExtractor\n",
    "from llama_index.core.schema import MetadataMode\n",
    "from llama_index.llms.openai import OpenAI\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo\", temperature=0.1)\n",
    "\n",
    "enrich_metadata_pipeline = IngestionPipeline(transformations=[SentenceSplitter(chunk_size=1024, chunk_overlap=20),\n",
    "                                                              TitleExtractor(llm=llm, metadata_mode=MetadataMode.EMBED),\n",
    "                                                              KeywordExtractor(llm=llm, metadata_mode=MetadataMode.EMBED),\n",
    "                                                             ])\n",
    "nodes = enrich_metadata_pipeline.run(documents=documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8ed67888-2e38-4e5c-9013-6ac97861f9e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[dict_keys(['page_label', 'file_name', 'file_path', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'document_title', 'excerpt_keywords']),\n",
       " dict_keys(['page_label', 'file_name', 'file_path', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'document_title', 'excerpt_keywords']),\n",
       " dict_keys(['page_label', 'file_name', 'file_path', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'document_title', 'excerpt_keywords']),\n",
       " dict_keys(['page_label', 'file_name', 'file_path', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'document_title', 'excerpt_keywords']),\n",
       " dict_keys(['page_label', 'file_name', 'file_path', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'document_title', 'excerpt_keywords']),\n",
       " dict_keys(['page_label', 'file_name', 'file_path', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'document_title', 'excerpt_keywords']),\n",
       " dict_keys(['page_label', 'file_name', 'file_path', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'document_title', 'excerpt_keywords']),\n",
       " dict_keys(['page_label', 'file_name', 'file_path', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'document_title', 'excerpt_keywords']),\n",
       " dict_keys(['page_label', 'file_name', 'file_path', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'document_title', 'excerpt_keywords']),\n",
       " dict_keys(['page_label', 'file_name', 'file_path', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'document_title', 'excerpt_keywords']),\n",
       " dict_keys(['file_path', 'file_name', 'file_size', 'creation_date', 'last_modified_date', 'document_title', 'excerpt_keywords']),\n",
       " dict_keys(['file_path', 'file_name', 'file_size', 'creation_date', 'last_modified_date', 'document_title', 'excerpt_keywords']),\n",
       " dict_keys(['file_path', 'file_name', 'file_size', 'creation_date', 'last_modified_date', 'document_title', 'excerpt_keywords']),\n",
       " dict_keys(['file_path', 'file_name', 'file_size', 'creation_date', 'last_modified_date', 'document_title', 'excerpt_keywords']),\n",
       " dict_keys(['file_path', 'file_name', 'file_size', 'creation_date', 'last_modified_date', 'document_title', 'excerpt_keywords']),\n",
       " dict_keys(['file_path', 'file_name', 'file_size', 'creation_date', 'last_modified_date', 'document_title', 'excerpt_keywords']),\n",
       " dict_keys(['file_path', 'file_name', 'file_size', 'creation_date', 'last_modified_date', 'document_title', 'excerpt_keywords']),\n",
       " dict_keys(['file_path', 'file_name', 'file_size', 'creation_date', 'last_modified_date', 'document_title', 'excerpt_keywords']),\n",
       " dict_keys(['page_label', 'file_name', 'file_path', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'document_title', 'excerpt_keywords']),\n",
       " dict_keys(['page_label', 'file_name', 'file_path', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'document_title', 'excerpt_keywords']),\n",
       " dict_keys(['page_label', 'file_name', 'file_path', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'document_title', 'excerpt_keywords']),\n",
       " dict_keys(['document_title', 'excerpt_keywords']),\n",
       " dict_keys(['document_title', 'excerpt_keywords'])]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[n.metadata.keys() for n in nodes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "990a274e-fcc0-45f9-831c-28b73635ee46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Tech Lead, Machine Learning, Robotics Engineering, Fraud Detection, Computer Vision',\n",
       " 'Robotics, AI, Reinforcement learning, SLAM, Neural networks',\n",
       " 'Software Reusability, Robotics, Component-Based Development, Refactoring Techniques, SLAM Framework',\n",
       " 'Component-Based Development, Refactoring Techniques, Reuse-oriented SLAM Framework, Robotics Applications, Software Reusability',\n",
       " 'SLAM, Reuse-oriented, Software product lines, Reusability, Domain analysis',\n",
       " 'SLAM, Reuse-Oriented, Inference Back-end, Platform-Specific, Map Representation',\n",
       " 'SLAM, Product Line Development, Derivation, Modularity, Extensibility',\n",
       " 'SLAM, reusability, standardization, software product line, interoperability',\n",
       " 'Software Product Line Engineering, SLAM Systems, Rao-Blackwellized Particle Filters, Brain-Based Systems, Robotics',\n",
       " 'SLAM, Software Product Line Engineering, Rao-Blackwellized Particle Filters, Brain-Based Systems, Robotics',\n",
       " 'document understanding, transformation, metadata extraction, embedding, query engine',\n",
       " 'Llama-Index, Document Understanding, Metadata Extraction, Embedding, Query Engine',\n",
       " 'Hybrid RAG Application, Llama-Index, Document Understanding, Metadata Extraction, Query Engine',\n",
       " 'FastAPI, Traefik, HTTPS, Docker, AWS',\n",
       " 'FastAPI, HTTPS, Traefik, Docker Compose, AWS EC2',\n",
       " 'Monty Hall Problem, Probability, Simulation, Decision Making, Game Show',\n",
       " 'Node Embedding, Pytorch, Graph Representation Learning, Shallow Embedding, Dot Product Similarity',\n",
       " 'node embedding, PyTorch, graph representation learning, dot product similarity, training',\n",
       " 'Semantic Segmentation, Convolutional Neural Networks, Deep Learning, Scene Understanding, Multi-scale Segmentation',\n",
       " 'Semantic Segmentation, Fully Convolutional Networks, DeepLabv3, Full-Resolution Residual Networks, Atrous Convolutions',\n",
       " 'deep learning, image recognition, semantic segmentation, convolutional neural networks, dataset',\n",
       " 'Machine Learning, Tech Lead, Data-Driven Products, Robotics, Python',\n",
       " 'Software Development, Tech Leadership, Machine Learning, Robotics, Data-Driven Products']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[n.metadata[\"excerpt_keywords\"] for n in nodes]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "925d3910-27cf-44d5-bf67-59ed2c5c1291",
   "metadata": {},
   "source": [
    "### Using pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "acdb0ea2-73ed-4494-b19a-a7e03e735692",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'indexes': [{'dimension': 1536,\n",
       "              'host': 'quickstart-index-es1tgmv.svc.aped-4627-b74a.pinecone.io',\n",
       "              'metric': 'euclidean',\n",
       "              'name': 'quickstart-index',\n",
       "              'spec': {'serverless': {'cloud': 'aws', 'region': 'us-east-1'}},\n",
       "              'status': {'ready': True, 'state': 'Ready'}}]}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from pinecone import Pinecone, ServerlessSpec, PodSpec\n",
    "from llama_index.vector_stores.pinecone import PineconeVectorStore\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.core.response.pprint_utils import pprint_source_node\n",
    "from llama_index.core import StorageContext\n",
    "\n",
    "pc = Pinecone(\n",
    "    api_key=os.environ.get(\"PINECONE_API_KEY\")\n",
    ")\n",
    "pc.list_indexes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6b3697b6-7165-4ba3-ba49-a4110b4c3796",
   "metadata": {},
   "outputs": [],
   "source": [
    "pinecone_index = pc.Index(\"quickstart-index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0971f8b9-b909-470e-acb7-fcc056595059",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dimension': 1536,\n",
       " 'index_fullness': 0.0,\n",
       " 'namespaces': {'': {'vector_count': 23}},\n",
       " 'total_vector_count': 23}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pinecone_index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6232957a-49c0-4a4f-bf28-5f4b577e3318",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = PineconeVectorStore(pinecone_index=pinecone_index)\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "396141e0-4983-4f85-aa50-9282d0ec1cce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating embeddings: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 23/23 [00:01<00:00, 16.02it/s]\n",
      "Upserted vectors: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 23/23 [00:01<00:00, 15.17it/s]\n"
     ]
    }
   ],
   "source": [
    "index = VectorStoreIndex(\n",
    "    nodes, storage_context=storage_context, show_progress=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1372eff9-f5a9-49a2-aeba-0524d998fa0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# set Logging to DEBUG for more detailed outputs\n",
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\"What is reusable SLAM?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "79940847-0642-4016-ba8f-cc3bc93e3483",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A reusable SLAM is a framework that emphasizes reusability as a primary requirement of SLAM software. It introduces structured development processes and creates a SLAM infrastructure with core separable modules implemented as encapsulated interchangeable components forming a software product line. The reusability of the framework is evaluated based on reuse-readiness levels criteria, resulting in improved modularity and reduction in development and deployment time and effort.\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b287fccc-4fc8-491c-88a0-ca35c116e9a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node ID: 9225c8d0-49c2-4def-85e0-cb899774b8d3\n",
      "Similarity: 0.258820891\n",
      "Text: Reuse-oriented SLAM Framework using Software Product Lines\n",
      "Mohamed A. Abdelhady∗, Douwe Dresscher∗and Jan F. Broenink∗ ∗Faculty\n",
      "of Electrical Engineering, Mathematics and Computer Science University\n",
      "of Twente, 7500 AE Enschede, The Netherlands Emails:\n",
      "m.adel.abdelhady@gmail.com, {d.dresscher, j.f.broenink }@utwente.nl\n",
      "Abstract —Simultaneous Loca...\n",
      "Node ID: 86e837bc-b907-4dfb-9177-2af65b442169\n",
      "Similarity: 0.275452733\n",
      "Text: commonly used perception algorithms and data structures. The\n",
      "results yield a perception framework with standalone atomic software\n",
      "components and harmonized interfaces that can easily be used to\n",
      "interchange algorithms and benchmark them. Thus, allowing the\n",
      "developers to decide in an early stage which is the most suitable\n",
      "algorithm for the applica...\n"
     ]
    }
   ],
   "source": [
    "for node in response.source_nodes:\n",
    "    pprint_source_node(node)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5ad392-9bba-496e-8d8a-9c1e982c3964",
   "metadata": {},
   "source": [
    "### Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "884487b1-ee60-4f84-b54c-f108304828c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating embeddings: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:01<00:00, 16.37it/s]\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import VectorStoreIndex\n",
    "# On a high-level, index can be created from documents directly, this will use a default node parser\n",
    "# index = VectorStoreIndex.from_documents(documents, show_progress=True)\n",
    "\n",
    "index = VectorStoreIndex(nodes, show_progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "859e1835-6832-45b8-b071-8ee085ef3b1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(index.index_struct.nodes_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "7d92db9c-d7ab-4267-8611-02c974bff309",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dc634446-09b3-48e3-9079-bcbb63643d06': RefDocInfo(node_ids=['f7d41b6a-e257-4210-9495-d0ef69f92262'], metadata={'page_label': '1', 'file_name': 'CV-M-Abdelhady.pdf', 'file_path': '/Users/mohamedadelabdelhady/workspace/kaggle-sandbox/llm-playground/data/CV-M-Abdelhady.pdf', 'file_type': 'application/pdf', 'file_size': 108188, 'creation_date': '2024-06-11', 'last_modified_date': '2024-06-11', 'document_title': '\"Tech Lead in Machine Learning and Robotics Engineering Resume\"', 'excerpt_keywords': 'Tech Lead, Machine Learning, Robotics Engineering, Fraud Detection, Computer Vision'}),\n",
       " '90e77d42-d332-4fd7-9091-05e89aa86980': RefDocInfo(node_ids=['63d896de-ff20-473d-ba52-b595089cd2c7'], metadata={'page_label': '2', 'file_name': 'CV-M-Abdelhady.pdf', 'file_path': '/Users/mohamedadelabdelhady/workspace/kaggle-sandbox/llm-playground/data/CV-M-Abdelhady.pdf', 'file_type': 'application/pdf', 'file_size': 108188, 'creation_date': '2024-06-11', 'last_modified_date': '2024-06-11', 'document_title': '\"Resume: Multifaceted Robotics and AI Professional\"', 'excerpt_keywords': 'Robotics, AI, Python, TensorFlow, Publications'}),\n",
       " '736c29ee-16db-4cfe-aec8-b14434a9eb22': RefDocInfo(node_ids=['f7867f2f-3723-4582-a6e9-fd102933ee6a', 'd6fca4db-2baa-4129-a538-b3519b2bcc2e', '0b7dcca8-d36b-4e3a-b089-dfb63945b946', '7b4c9a64-8c8b-4ccc-8400-a8a7348716e6', '43cb3779-4e11-44b6-bd48-497639f8f727', 'dba0ae73-f855-4077-8368-313f361bee11', 'a0313768-f6c9-44b2-ae0d-31069a66f835', 'e48616fb-c4dc-44ed-9998-984f15180576', 'e44a7cc9-f145-456e-933c-821caaa0afde', 'c2564e5e-d1b3-4cbe-8e8b-41bf31ff7b4f', 'ddfd566c-8a7d-4cd2-90a2-c5e93f017975', 'a713a163-59c6-4268-84e0-6f78715204e8', '352e46a5-ed8b-41b8-bd40-f3dbf356a32f', '07bc095e-30a1-4ee3-8836-65558d68efad', '75ec3fc8-a2e3-429a-bc44-6a20f8e52d90', '3721861e-5e85-4641-98f7-785de03cc7c4', '52e5b565-f000-4fc1-bbb2-cb4809bfa8de', 'd4cf1a13-c368-408a-a673-228c973e683a'], metadata={'file_path': '/Users/mohamedadelabdelhady/workspace/kaggle-sandbox/llm-playground/data/paul_graham_essay.txt', 'file_name': 'paul_graham_essay.txt', 'file_type': 'text/plain', 'file_size': 75041, 'creation_date': '2024-05-03', 'last_modified_date': '2024-05-02', 'document_title': 'A Journey of Exploration and Transformation: From Writing and Programming to Art and Philosophy', 'excerpt_keywords': 'writing, programming, philosophy, AI, college'}),\n",
       " '919f1ef8-9ab9-48f9-8488-e5b523ae9503': RefDocInfo(node_ids=['c3a89c37-809d-4f8b-b077-b6baad787e14'], metadata={'foo': 'bar', 'category': 'documentation', 'document_title': 'Storing Indexed Data with the .persist() Method: A Comprehensive Guide', 'excerpt_keywords': 'Storing, Indexed Data, .persist() Method, Comprehensive Guide, Disk Storage'}),\n",
       " 'https://docs.pinecone.io/home': RefDocInfo(node_ids=['a0754d56-b619-4267-aea0-ab1b3c6be53a'], metadata={'document_title': 'Pinecone Documentation and Resources: A Comprehensive Guide', 'excerpt_keywords': 'Pinecone, Documentation, Resources, Guides, Integrations'})}"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# no direct way to show the actual vector embeddings :/\n",
    "index.ref_doc_info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33869137-e86d-4417-863f-f08f627ddfa0",
   "metadata": {},
   "source": [
    "### Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "81e47ca5-4675-48e3-8ac9-ce6752c0835a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this will overwrite all the json files in storage\n",
    "index.storage_context.persist(persist_dir=\"./storage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "ee21046a-1bbc-4753-b1d9-330a22bf0c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from llama_index.core import StorageContext\n",
    "from llama_index.vector_stores.pinecone import PineconeVectorStore\n",
    "\n",
    "pc = Pinecone(\n",
    "    api_key=os.environ.get(\"PINECONE_API_KEY\")\n",
    ")\n",
    "pinecone_index = pc.Index(\"quickstart-index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "a75e610a-c44b-46b0-bc10-ea1a329cd746",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = PineconeVectorStore(pinecone_index=pinecone_index)\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "# index = VectorStoreIndex(nodes, storage_context=storage_context, show_progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "8cea1a3f-9ff2-4a25-a806-5b4b5a9523e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# not sure if this works\n",
    "index.storage_context.add_vector_store(vector_store, namespace=\"default\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8dd9ce-d804-43d3-9468-9b20f9a0081c",
   "metadata": {},
   "source": [
    "### Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "06ee32d7-e7de-4617-b501-2db6bd0ef719",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import StorageContext, load_index_from_storage\n",
    "\n",
    "storage_context = StorageContext.from_defaults(persist_dir=\"./storage\")\n",
    "index = load_index_from_storage(storage_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "bbf068d3-9160-423d-a54e-a2fb044d52e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'5daac5eb-7e75-44d8-aee3-c308f359d2b3': RefDocInfo(node_ids=['b7cd3755-8335-40e5-875e-b06a357ba38f', '8ebbf89f-3196-4b9f-865b-f5fde85479dc', '6c841bcb-3c1c-4189-ae34-979575c95693', '9c9bae0a-f961-47b3-96f3-5c5675b06c74', '1ee49e3d-2b90-43c5-b735-46552a9f71b8', '438ba729-223f-4ffe-bbc8-a3aabb9331b0', 'c13c2f90-42eb-4356-9b52-926376f215b6', '8a135b25-ecd0-4bbc-b907-b17722419151', '0db7e674-47a3-487c-9393-a47ad2104545', '970e3b9b-d05d-412c-8141-ba9454774491', 'df8070e7-3cec-4375-ad30-609f39907071', '494f771d-d55c-43d4-a2d7-08bfb9c471fc', '6968eafd-98af-4a6c-94ed-18a173174c0f', '34733c08-6120-412f-80f8-7cd6c1ee7488', '63d0a688-7cf1-48c2-8c12-f4c03fd71de9', '40c7ceb5-420a-4cb2-94c2-23bab804dc31', '20cc598b-a66e-44e6-927a-4a4282ce7993', 'b6b1f44b-9cf4-45ae-8985-59d21430ee0c'], metadata={'file_path': '/Users/mohamedadelabdelhady/workspace/kaggle-sandbox/llm-playground/data/paul_graham_essay.txt', 'file_name': 'paul_graham_essay.txt', 'file_type': 'text/plain', 'file_size': 75041, 'creation_date': '2024-05-03', 'last_modified_date': '2024-05-02', 'document_title': 'A Journey of Exploration: Writing, Programming, Philosophy, Art, and Language Learning', 'excerpt_keywords': 'writing, programming, philosophy, art, language learning'}),\n",
       " 'd9cf5ef1-06c3-4f09-9217-4ed120f8f681': RefDocInfo(node_ids=['335a9ddb-3a4e-410a-bde3-80143a7bea83'], metadata={'foo': 'bar', 'category': 'documentation', 'document_title': 'Storing Indexed Data with the .persist() Method: A Comprehensive Guide', 'excerpt_keywords': 'Storing, Indexed Data, .persist() Method, Comprehensive Guide, Disk Storage'}),\n",
       " 'https://docs.pinecone.io/home': RefDocInfo(node_ids=['ef41f6b7-ff90-4613-a121-f9be2f3c406e'], metadata={'document_title': '\"Pinecone Documentation and Resources: A Comprehensive Guide\"', 'excerpt_keywords': 'Pinecone, Documentation, Resources, Guides, Integrations'})}"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.ref_doc_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "da6d603f-5832-4cd8-87c2-63e2ed902312",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(index.index_struct.nodes_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980fb10f-3c19-4019-80a3-336cbd0dfd86",
   "metadata": {},
   "source": [
    "### Querying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "cff93ef1-19ab-4de7-9184-8ddd4685ec2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex, get_response_synthesizer, StorageContext, load_index_from_storage\n",
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core.postprocessor import SimilarityPostprocessor, KeywordNodePostprocessor\n",
    "from llama_index.core.response_synthesizers import ResponseMode\n",
    "from llama_index.core.response.pprint_utils import pprint_source_node\n",
    "\n",
    "storage_context = StorageContext.from_defaults(persist_dir=\"./storage\")\n",
    "index = load_index_from_storage(storage_context)\n",
    "\n",
    "retriever = VectorIndexRetriever(index=index, similarity_top_k=5)\n",
    "response_synthesizer = get_response_synthesizer(response_mode=ResponseMode.COMPACT)\n",
    "\n",
    "# assemble the query engine\n",
    "query_engine = RetrieverQueryEngine(retriever=retriever, response_synthesizer=response_synthesizer, node_postprocessors=[SimilarityPostprocessor(similarity_cutoff=0.8)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "98ca1175-45d1-4bc7-881a-481c517ed1a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paul Graham studied philosophy in college before switching to AI.\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"Where did paul graham study?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "6472e93f-7855-4e09-a562-49edd422d601",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node ID: 0b7dcca8-d36b-4e3a-b089-dfb63945b946\n",
      "Similarity: 0.8290162462369435\n",
      "Text: There were some surplus Xerox Dandelions floating around the\n",
      "computer lab at one point. Anyone who wanted one to play around with\n",
      "could have one. I was briefly tempted, but they were so slow by\n",
      "present standards; what was the point? No one else wanted one either,\n",
      "so off they went. That was what happened to systems work.  I wanted\n",
      "not just to bui...\n",
      "Node ID: 43cb3779-4e11-44b6-bd48-497639f8f727\n",
      "Similarity: 0.8274202550584204\n",
      "Text: Our teacher, professor Ulivi, was a nice guy. He could see I\n",
      "worked hard, and gave me a good grade, which he wrote down in a sort\n",
      "of passport each student had. But the Accademia wasn't teaching me\n",
      "anything except Italian, and my money was running out, so at the end\n",
      "of the first year I went back to the US.  I wanted to go back to RISD,\n",
      "but I was ...\n",
      "Node ID: f7867f2f-3723-4582-a6e9-fd102933ee6a\n",
      "Similarity: 0.814594074107659\n",
      "Text: What I Worked On  February 2021  Before college the two main\n",
      "things I worked on, outside of school, were writing and programming. I\n",
      "didn't write essays. I wrote what beginning writers were supposed to\n",
      "write then, and probably still are: short stories. My stories were\n",
      "awful. They had hardly any plot, just characters with strong feelings,\n",
      "which I ...\n",
      "Node ID: c2564e5e-d1b3-4cbe-8e8b-41bf31ff7b4f\n",
      "Similarity: 0.8118458244502096\n",
      "Text: So I tried to paint, but I just didn't seem to have any energy\n",
      "or ambition. Part of the problem was that I didn't know many people in\n",
      "California. I'd compounded this problem by buying a house up in the\n",
      "Santa Cruz Mountains, with a beautiful view but miles from anywhere. I\n",
      "stuck it out for a few more months, then in desperation I went back to\n",
      "New...\n",
      "Node ID: 3721861e-5e85-4641-98f7-785de03cc7c4\n",
      "Similarity: 0.8091210738626499\n",
      "Text: McCarthy's 1960 Lisp did nothing more than interpret Lisp\n",
      "expressions. It was missing a lot of things you'd want in a\n",
      "programming language. So these had to be added, and when they were,\n",
      "they weren't defined using McCarthy's original axiomatic approach.\n",
      "That wouldn't have been feasible at the time. McCarthy tested his\n",
      "interpreter by hand-simulati...\n"
     ]
    }
   ],
   "source": [
    "for node in response.source_nodes:\n",
    "    pprint_source_node(node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "019df92d-0705-48ef-817d-42b1200dba25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The simplest way to store indexed data is to use the built-in .persist() method of every Index, which writes all the data to disk at the location specified.\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"what is the simplest way to store indexed data?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "e521bbae-9d51-4411-99a2-86df0fde146e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node ID: c3a89c37-809d-4f8b-b077-b6baad787e14\n",
      "Similarity: 0.8419226761880413\n",
      "Text: The simplest way to store your indexed data is to use the built-\n",
      "in .persist() method of every Index, which writes all the data to disk\n",
      "at the location specified. This works for any type of index.\n"
     ]
    }
   ],
   "source": [
    "for node in response.source_nodes:\n",
    "    pprint_source_node(node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "d8e3d624-0e22-434b-8d9e-5d47ab27319e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Pinecone Documentation and Resources provide a comprehensive guide for users, offering practical guides, detailed information about the Pinecone API, SDKs, and architecture, hands-on examples and sample apps, details on third-party integrations, Pinecone utilities and reference architectures, troubleshooting guides, and news about features and changes in Pinecone and related tools.\n",
      "Node ID: a0754d56-b619-4267-aea0-ab1b3c6be53a\n",
      "Similarity: 0.89678371872626\n",
      "Text: [Pinecone Docs home page![light logo](https://mintlify.s3-us-\n",
      "west-1.amazonaws.com/pinecone-2/logo/light.png)![dark\n",
      "logo](https://mintlify.s3-us-\n",
      "west-1.amazonaws.com/pinecone-2/logo/dark.png)](/)  Latest  Search or\n",
      "ask...    * [Sign up\n",
      "free](https://app.pinecone.io/?sessionType=signup)   *\n",
      "[Status](https://status.pinecone.io)   * [Support](http...\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"Pinecone Documentation and Resources: A Comprehensive Guide\")\n",
    "print(response)\n",
    "for node in response.source_nodes:\n",
    "    pprint_source_node(node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "e35dc151-263b-40ee-ad95-22d2608e99c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty Response\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"Who is Mohamed A. Abdelhady ?\")\n",
    "print(response)\n",
    "for node in response.source_nodes:\n",
    "    pprint_source_node(node)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
