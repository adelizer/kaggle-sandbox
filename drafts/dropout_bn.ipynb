{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "dropout-bn.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5hXVxk4dgaAH",
        "colab_type": "text"
      },
      "source": [
        "# Understanding the Disharmony between Dropout and Batch Normalization\n",
        "\n",
        "Why two of the most commonly used techniques perform worse when applied together?\n",
        "\n",
        "* Dropout: \n",
        "    * Used to avoid overfitting \n",
        "    * Simple to implement\n",
        "    * Widely adopted\n",
        "* Batch Normalization:\n",
        "    * Enables faster training / higher learning rates\n",
        "    * Reduce the dependency over careful initialization\n",
        "    * Widely adopted\n",
        "\n",
        "\n",
        "As it turns out, applying dropout before batch normalization leads to a \"variance shift\" phenomenon, which is the key to why these two techniques should be combined with care. \n",
        "This variance shift is due to the different behavior that dropout exhibits between training and testing phases. The main intuition suggests that Batch norm learns some statistics during training that are not kept during testing.\n",
        "\n",
        "Main reference: https://arxiv.org/abs/1801.05134\n",
        "\n",
        "**The notebook will cover the following points:**\n",
        " 1- Dropout\n",
        " 2- BatchNorm\n",
        " 3- Combined Dropout + BatchNorm and calculation of the variance shift \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "INsif4YfS4Tz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import keras\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mHZUyh2ATSJ4",
        "colab_type": "code",
        "outputId": "6a956bce-40e5-41ec-bf7e-bd23ad7bbe23",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 260
        }
      },
      "source": [
        "\"\"\"\n",
        "p is the dropout keep probability, which is equivalent to 1-dropout_rate\n",
        "For dropout, there are two ways we can handle scaling of the activations. \n",
        "During test the activation can be scaled down by a factor of p\n",
        "\n",
        "Or (Inverse dropout) we can scale the activations up during training by a factor\n",
        "of p and keep the testing activation the same\n",
        "\"\"\"\n",
        "\n",
        "n = 5 # the dimension of the features (k in the paper)\n",
        "dropout_rate = 0.2\n",
        "inputs = keras.Input(shape=(n,))\n",
        "# x = keras.layers.Lambda(lambda x: x)(inputs)\n",
        "outputs = keras.layers.Dropout(rate=dropout_rate)(inputs)\n",
        "dropout_model = keras.Model(inputs, outputs)\n",
        "dropout_model.compile(loss='mse', optimizer='adam')\n",
        "print(dropout_model.summary())\n",
        "print(1/(1-dropout_rate))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_6\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_6 (InputLayer)         (None, 5)                 0         \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 5)                 0         \n",
            "=================================================================\n",
            "Total params: 0\n",
            "Trainable params: 0\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "1.25\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zCRfclZjT2F5",
        "colab_type": "code",
        "outputId": "45ae841d-f530-4d97-dcbb-6f938f36298c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "x_test = np.ones((1,n))\n",
        "pred = dropout_model.predict(x_test)\n",
        "print(pred)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1. 1. 1. 1. 1.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g2IxWJTmPieU",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "---\n",
        "# **Batch Normalization case**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a6YBoVEhjzX2",
        "colab_type": "code",
        "outputId": "400a00b7-7f79-4fe9-bf70-371b070fdfd7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        }
      },
      "source": [
        "inputs = keras.Input(shape=(n,))\n",
        "outputs = keras.layers.BatchNormalization()(inputs)\n",
        "bn_model = keras.Model(inputs, outputs)\n",
        "bn_model.compile(loss='mse', optimizer='adam')\n",
        "bn_model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "Model: \"model_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_4 (InputLayer)         (None, 5)                 0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 5)                 20        \n",
            "=================================================================\n",
            "Total params: 20\n",
            "Trainable params: 10\n",
            "Non-trainable params: 10\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oRkgPviLn3Cj",
        "colab_type": "code",
        "outputId": "f6af7405-0b6f-499a-969d-9c8670db3947",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "bn_layer = bn_model.layers[1]\n",
        "print(bn_layer.get_config()['name'])\n",
        "print(\"[gamma(scale), beta(shift), running mean , running variance]\\n\")\n",
        "gamma, beta, mean, var = bn_layer.get_weights()\n",
        "print(\"gamma (scale) \", gamma)\n",
        "print(\"beta (shift) \", beta)\n",
        "print(\"moving mean \", mean)\n",
        "print(\"moving_variance \", var)\n",
        "# x_bn = x*gamma + beta\n",
        "# with the same input samp  le we train beta parameter to match the running mean"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "batch_normalization_1\n",
            "[gamma(scale), beta(shift), running mean , running variance]\n",
            "\n",
            "gamma (scale)  [1. 1. 1. 1. 1.]\n",
            "beta (shift)  [0. 0. 0. 0. 0.]\n",
            "moving mean  [0. 0. 0. 0. 0.]\n",
            "moving_variance  [1. 1. 1. 1. 1.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t9dIMDCKj70F",
        "colab_type": "code",
        "outputId": "860aaf0b-1b0d-41dc-e4e0-4e5afde5d50f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "mu = 5\n",
        "sigma = 2\n",
        "x_train = np.random.normal(mu,sigma,size=(10000,n))\n",
        "print(\"mean: \", np.mean(x_train,axis=0))\n",
        "print(\"std: \", np.std(x_train, axis=0))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mean:  [5.02197837 5.0263782  5.04208676 5.00473362 5.00503994]\n",
            "std:  [2.0145153  1.99571036 1.99048178 1.98553206 1.98840974]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "paX72pXckDOU",
        "colab_type": "code",
        "outputId": "0755c24f-88a2-4c25-f382-6e099c4f4714",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        }
      },
      "source": [
        "history = bn_model.fit(x_train,x_train, epochs=50, verbose=0)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M-EwqoRAjIou",
        "colab_type": "code",
        "outputId": "32784c12-4a72-4aee-f5ac-2c3335c18f2f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "gamma, beta, mean, var = bn_layer.get_weights()\n",
        "print(\"gamma (scale) \", gamma)\n",
        "print(\"beta (shift) \", beta)\n",
        "print(\"moving mean \", mean)\n",
        "print(\"moving_variance \", var)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "gamma (scale)  [1.9716144 1.9501512 1.945204  1.9310975 1.9399271]\n",
            "beta (shift)  [5.0193267 5.023732  5.041845  5.010377  5.0020723]\n",
            "moving mean  [4.999528  5.0187097 5.0573626 5.0396338 4.978292 ]\n",
            "moving_variance  [4.174094  3.9469488 4.035455  3.889659  3.873004 ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hoMHK0_Lj7xk",
        "colab_type": "code",
        "outputId": "71e78a56-7fc6-418d-dc8a-309999a17a8b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "pred = bn_model.predict(np.ones((1,n)))\n",
        "print(pred)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1.1601241 1.0794395 1.1135061 1.0554851 1.0810254]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LwiTlrUwSU0R",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "# **Dropout + Batch Normalization**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FAjDzHobyEre",
        "colab_type": "code",
        "outputId": "ce18b8b3-c195-4f3d-d14c-dee82824a614",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 260
        }
      },
      "source": [
        "inputs = keras.Input(shape=(n,))\n",
        "x = keras.layers.Dropout(dropout_rate)(inputs) # <== To have the discrepancy between train and test\n",
        "outputs = keras.layers.BatchNormalization()(x)\n",
        "model = keras.Model(inputs, outputs=outputs)\n",
        "model.compile(loss='mse', optimizer='adam')\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_5 (InputLayer)         (None, 5)                 0         \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 5)                 0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 5)                 20        \n",
            "=================================================================\n",
            "Total params: 20\n",
            "Trainable params: 10\n",
            "Non-trainable params: 10\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5KpmVy4ayRJz",
        "colab_type": "code",
        "outputId": "7098eff6-b64f-499d-ff59-d931c5e08234",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "bn_layer = model.layers[2]\n",
        "gamma, beta, mean, var = bn_layer.get_weights()\n",
        "print(\"gamma (scale) \", gamma)\n",
        "print(\"beta (shift) \", beta)\n",
        "print(\"moving mean \", mean)\n",
        "print(\"moving_variance \", var)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "gamma (scale)  [1. 1. 1. 1. 1.]\n",
            "beta (shift)  [0. 0. 0. 0. 0.]\n",
            "moving mean  [0. 0. 0. 0. 0.]\n",
            "moving_variance  [1. 1. 1. 1. 1.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aSE_ReAkzLdn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "history = model.fit(x_train, x_train, epochs=100, verbose=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XaDp0RZhyw90",
        "colab_type": "code",
        "outputId": "378839e2-6902-46e3-d592-e9738f1368ca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "pred = model.predict(np.ones((1,n)))\n",
        "print(pred)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[3.541892  3.6039495 3.6400888 3.6278784 3.6181204]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mBd5FA2Umdes",
        "colab_type": "code",
        "outputId": "ff7b3e4b-cec5-445d-f52f-09df6c6baa91",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "bn_layer = model.layers[2]\n",
        "gamma, beta, mean, var = bn_layer.get_weights()\n",
        "print(\"gamma (scale) \", gamma)\n",
        "print(\"beta (shift) \", beta)\n",
        "print(\"moving mean \", mean)\n",
        "print(\"moving_variance \", var)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "gamma (scale)  [1.208375  1.1724666 1.1604689 1.1533899 1.1569933]\n",
            "beta (shift)  [5.029295  5.030495  5.0373297 5.0059257 5.0011787]\n",
            "moving mean  [5.126892  5.070994  5.0210896 5.0042624 5.0066295]\n",
            "moving_variance  [11.2396755 11.194161  11.15252   11.231312  11.233118 ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UjGfJzHpqwUg",
        "colab_type": "text"
      },
      "source": [
        "The equation for the variance is: \n",
        "\n",
        "$c = E[x_k]$\n",
        "\n",
        "$v = Var[x_k]$\n",
        "\n",
        "$Var^{Train}(x_k) = \\frac{1}{p}(c^2 + v) - c^2$\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s9MKgz03m19Q",
        "colab_type": "code",
        "outputId": "6930358a-13cf-4d89-8f94-5b328715f104",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "c = np.mean(x_train, axis=0)\n",
        "v = np.var(x_train, axis=0)\n",
        "moving_var_train = 1/(1-dropout_rate) * (c**2 + v) - c**2 \n",
        "print(moving_var_train)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[11.37790653 11.29469424 11.30818186 11.18976161 11.2048228 ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uqw3m8Sur57Y",
        "colab_type": "code",
        "outputId": "e6fdc17b-1370-41c7-b1d2-d68a07c9ffe6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "\"\"\"\n",
        "An interesting experiment is to try to undo this variance shift since BN already\n",
        "learning the correct mean and we can access the information about the dropout\n",
        "retain rate.\n",
        "However, this is only limited to the case of Dropout -> BN and not \n",
        "Dropout -> Conv ->  BN. \n",
        "Additionally, it turns out that BN learns a different scale parameter gamma that\n",
        "needs to be corrected as well. Although the suggested correction already reduces\n",
        "the shift \n",
        "\"\"\"\n",
        "\n",
        "restored_var = (mean**2+var)*(1-dropout_rate) - mean**2\n",
        "print(restored_var)\n",
        "bn_layer.set_weights([gamma, beta, mean, restored_var])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[3.7347355 3.812334  3.8797493 3.9765224 3.9732265]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RYYWkNGfr5_0",
        "colab_type": "code",
        "outputId": "5c32cb66-cb6d-4f3d-8823-205a210a3619",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "pred = model.predict(np.ones((1,n)))\n",
        "print(pred)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[2.4491937 2.5862288 2.6685781 2.690172  2.6758535]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l2x_hNAMFDUr",
        "colab_type": "text"
      },
      "source": [
        "Guidelines & Recommendations:\n",
        "\n",
        "1- In modern CNNs batch norma and dropout are not recommended to be combined due to the variance shift phenomenon\n",
        "\n",
        "2- The severity of the variance shift depends on the dropout rate and feature dimensions\n",
        "\n",
        "3- Apply dropout after the last BN layer\n",
        "\n",
        "4- Adjusting the moving mean and variance by passing the training data during test is not enough\n",
        "\n",
        "5- New form of Dropout UDrop can help\n",
        "\n",
        "* Variance shift can lead to numerical disturbances that will be amplified and lead to misclassification\n",
        "\n",
        "\n"
      ]
    }
  ]
}